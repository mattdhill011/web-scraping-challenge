{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82fab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7f6853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "INFO:WDM:====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 103.0.5060\n",
      "INFO:WDM:Current google-chrome version is 103.0.5060\n",
      "[WDM] - Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "INFO:WDM:Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\wallh\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n",
      "INFO:WDM:Driver [C:\\Users\\wallh\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# First we go to the redplanetscience site and get the news titles\n",
    "\n",
    "target_site = \"https://redplanetscience.com/\"\n",
    "\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# In case this code runs into a problem, we want to make sure we still close the browser, so we use try/except/finally  \n",
    "\n",
    "try:\n",
    "    browser.visit(target_site)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    # I sometimes ran into an error when the page didn't load immediately, so we sleep for 5 seconds\n",
    "    # The rest of them we'll only sleep for 1 second since it doesn't seem to need to load as much\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    # We're interested only in the first (most recent) piece of content so we can use .find here\n",
    "    news_title = soup.find('div', class_='content_title').text\n",
    "    news_p = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "\n",
    "    # Next up is spaceimages\n",
    "\n",
    "    target_site = \"https://spaceimages-mars.com/\"\n",
    "\n",
    "    browser.visit(target_site)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    featured_image_url = soup.find('a', class_='showimg fancybox-thumbs')['href']\n",
    "\n",
    "    # Now for mars facts\n",
    "\n",
    "    target_site = \"https://galaxyfacts-mars.com/\"\n",
    "\n",
    "    browser.visit(target_site)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    tables = pd.read_html(target_site)\n",
    "\n",
    "    # pd.read_html gives us a list of 2 tables, the first one is comparing earth and mars, we want the second one \n",
    "    mars_facts = tables[1]\n",
    "    \n",
    "    # Now to get the images of the martian hemispheres\n",
    "    \n",
    "    target_site = \"https://marshemispheres.com/\"\n",
    "    \n",
    "    browser.visit(target_site)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    # This gets a list of links in the web page and adds them to a list, excluding the back link\n",
    "    links = []\n",
    "\n",
    "    for link in soup.find_all('h3'):\n",
    "        link_address = link.text\n",
    "        if link_address not in links:\n",
    "            links.append(link_address)\n",
    "\n",
    "    # We take out the back button\n",
    "    links.remove(\"Back\")\n",
    "\n",
    "    hemisphere_image_urls = []\n",
    "\n",
    "    # Looping through the list we have we click on each link taking us to the next page,\n",
    "    # then find the button labeled original to get the link for that\n",
    "\n",
    "    for link in links:\n",
    "        browser.find_by_css('h3').links.find_by_partial_text(link).click()\n",
    "        image_link = browser.links.find_by_text(\"Original\")['href']\n",
    "    \n",
    "        # Then we make a dictionary to hold what we found and append it to the hemisphere_image_urls\n",
    "        image_dict = {\"title\":link, \"img_url\":image_link}\n",
    "        hemisphere_image_urls.append(image_dict)\n",
    "    \n",
    "        time.sleep(1)\n",
    "        browser.back()\n",
    "        time.sleep(1)\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")\n",
    "    \n",
    "finally:\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8414125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA's InSight Flexes Its Arm While Its 'Mole' Hits Pause\n",
      "Now that the lander's robotic arm has helped the mole get underground, it will resume science activities that have been on hold.\n",
      "image/featured/mars3.jpg\n",
      "                      0                              1\n",
      "0  Equatorial Diameter:                       6,792 km\n",
      "1       Polar Diameter:                       6,752 km\n",
      "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
      "3                Moons:          2 ( Phobos & Deimos )\n",
      "4       Orbit Distance:       227,943,824 km (1.38 AU)\n",
      "5         Orbit Period:           687 days (1.9 years)\n",
      "6  Surface Temperature:                   -87 to -5 °C\n",
      "7         First Record:              2nd millennium BC\n",
      "8          Recorded By:           Egyptian astronomers\n",
      "[{'title': 'Cerberus Hemisphere Enhanced', 'img_url': 'https://marshemispheres.com/images/cerberus_enhanced.tif'}, {'title': 'Schiaparelli Hemisphere Enhanced', 'img_url': 'https://marshemispheres.com/images/schiaparelli_enhanced.tif'}, {'title': 'Syrtis Major Hemisphere Enhanced', 'img_url': 'https://marshemispheres.com/images/syrtis_major_enhanced.tif'}, {'title': 'Valles Marineris Hemisphere Enhanced', 'img_url': 'https://marshemispheres.com/images/valles_marineris_enhanced.tif'}]\n"
     ]
    }
   ],
   "source": [
    "print(news_title)\n",
    "print(news_p)\n",
    "print(featured_image_url)\n",
    "print(mars_facts)\n",
    "print(hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3871b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape():\n",
    "\n",
    "    # First we go to the redplanetscience site and get the news titles\n",
    "\n",
    "    target_site = \"https://redplanetscience.com/\"\n",
    "\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "    mission_to_mars = {}\n",
    "\n",
    "    # In case this code runs into a problem, we want to make sure we still close the browser, so we use try/except/finally  \n",
    "\n",
    "    try:\n",
    "        browser.visit(target_site)\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        # I sometimes ran into an error when the page didn't load immediately, so we sleep for 5 seconds\n",
    "        # The rest of them we'll only sleep for 1 second since it doesn't seem to need to load as much\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        # We're interested only in the first (most recent) piece of content so we can use .find here\n",
    "        news_title = soup.find('div', class_='content_title').text\n",
    "        news_p = soup.find('div', class_='article_teaser_body').text\n",
    "\n",
    "        mission_to_mars[\"headline\"] = news_title\n",
    "        mission_to_mars[\"news\"] = news_p\n",
    "\n",
    "\n",
    "        # Next up is spaceimages\n",
    "\n",
    "        target_site = \"https://spaceimages-mars.com/\"\n",
    "\n",
    "        browser.visit(target_site)\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        featured_image_url = soup.find('a', class_='showimg fancybox-thumbs')['href']\n",
    "\n",
    "        mission_to_mars[\"featured\"] = target_site + featured_image_url\n",
    "\n",
    "        \n",
    "\n",
    "        # Now for mars facts\n",
    "\n",
    "        target_site = \"https://galaxyfacts-mars.com/\"\n",
    "\n",
    "        browser.visit(target_site)\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        tables = pd.read_html(target_site)\n",
    "\n",
    "        # pd.read_html gives us a list of 2 tables, the first one is comparing earth and mars, we want the second one \n",
    "        mars_facts = tables[1]\n",
    "\n",
    "        mission_to_mars[\"facts\"] = mars_facts\n",
    "    \n",
    "        # Now to get the images of the martian hemispheres\n",
    "    \n",
    "        target_site = \"https://marshemispheres.com/\"\n",
    "    \n",
    "        browser.visit(target_site)\n",
    "        html = browser.html\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        # This gets a list of links in the web page and adds them to a list, excluding the back link\n",
    "        links = []\n",
    "\n",
    "        for link in soup.find_all('h3'):\n",
    "            link_address = link.text\n",
    "            if link_address not in links:\n",
    "                links.append(link_address)\n",
    "\n",
    "        # We take out the back button\n",
    "        links.remove(\"Back\")\n",
    "\n",
    "        hemisphere_image_urls = []\n",
    "\n",
    "        # Looping through the list we have we click on each link taking us to the next page,\n",
    "        # then find the button labeled original to get the link for that\n",
    "\n",
    "        for link in links:\n",
    "            browser.find_by_css('h3').links.find_by_partial_text(link).click()\n",
    "            image_link = browser.links.find_by_text(\"Original\")['href']\n",
    "    \n",
    "            # Then we make a dictionary to hold what we found and append it to the hemisphere_image_urls\n",
    "            image_dict = {\"title\":link, \"img_url\":image_link}\n",
    "            hemisphere_image_urls.append(image_dict)\n",
    "    \n",
    "            time.sleep(1)\n",
    "            browser.back()\n",
    "            time.sleep(1)\n",
    "\n",
    "        mission_to_mars[\"images\"] = hemisphere_image_urls\n",
    "    \n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "    \n",
    "    finally:\n",
    "        browser.quit()\n",
    "    \n",
    "    \n",
    "    return mission_to_mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9b577a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "INFO:WDM:====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 103.0.5060\n",
      "INFO:WDM:Current google-chrome version is 103.0.5060\n",
      "[WDM] - Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "INFO:WDM:Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\wallh\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n",
      "INFO:WDM:Driver [C:\\Users\\wallh\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "mission_to_mars = scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ce7f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars3.jpg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mission_to_mars[\"featured\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
